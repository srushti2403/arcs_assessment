# -*- coding: utf-8 -*-
"""ARCS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xbUNT16VhI3d2zvAw1u62xk5Mb2aTipl
"""

!pip install transformers torch

from transformers import AutoTokenizer, AutoModelForCausalLM

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Define model name
model_name = "allenai/longformer-base-4096"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load the model (for sequence classification)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

print(f"Using Longformer model: {model_name}")

from google.colab import files

# Upload PDF files
uploaded = files.upload()

#!pip install PyPDF2 pdfplumber

!sudo apt install tesseract-ocr
!pip install pytesseract pillow pdfplumber

# Extracting text from file
from PIL import Image
from pytesseract import image_to_string
import pdfplumber

def extract_text_with_ocr(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            # Convert page to an image
            image = page.to_image(resolution=300).original
            # Extract text using Tesseract OCR
            text += image_to_string(image)
    return text

extracted_text = extract_text_with_ocr("/content/1.pdf")
print(extracted_text[:500])  # Inspect the first 500 characters of the OCR result

print(f"Vocabulary size: {tokenizer.vocab_size}")

from transformers import AutoTokenizer

# Load the tokenizer for Longformer
model_name = "allenai/longformer-base-4096"  # Replace with Longformer model name
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Tokenize the extracted text
inputs = tokenizer(
    extracted_text,
    return_tensors="pt",  # Return PyTorch tensors
    truncation=True,      # Ensure the input fits within the token limit
    max_length=4096       # Longformer's token limit
)

# Count the number of tokens
num_tokens = inputs["input_ids"].shape[1]  # Second dimension gives token count

print(f"Number of tokens in the extracted text: {num_tokens}")

def chunk_text(text, tokenizer, max_tokens):
    """
    Splits text into chunks based on the token limit.
    """
    tokens = tokenizer.encode(text, truncation=False)  # Tokenize the entire text without truncation
    chunks = []

    # Split tokens into chunks of max_tokens
    for i in range(0, len(tokens), max_tokens):
        chunks.append(tokens[i:i + max_tokens])

    return chunks

# Split text into manageable chunks for Longformer
max_tokens = 4096  # Longformer's maximum token limit
tokenized_chunks = chunk_text(extracted_text, tokenizer, max_tokens)

# Decode the first chunk to verify the content
decoded_chunks = [tokenizer.decode(chunk) for chunk in tokenized_chunks]

print(f"Number of chunks: {len(decoded_chunks)}")
print(f"First chunk: {decoded_chunks[0][:500]}")  # Preview the first chunk

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load the tokenizer and model
model_name = "allenai/longformer-base-4096"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Define the task-specific prompt
text = decoded_chunks[0]  # First chunk
prompt = (
    f"Determine if the following text supports or contradicts the principle of "
    f"'contra proferentem' (a legal doctrine favoring the interpretation against the drafter "
    f"in cases of ambiguous contract terms). Respond with 'YES' if it supports or 'NO' if it contradicts. "
    f"Text:\n\n{text}"
)

# Tokenize the input
inputs = tokenizer(
    prompt,
    return_tensors="pt",
    truncation=True,  # Ensure input fits the model's token limit
    padding="max_length",  # Pad to max length for consistency
    max_length=4096
)

# Process the chunk with Longformer
outputs = model(**inputs)

# Extract logits and determine the predicted label
logits = outputs.logits
predicted_label = logits.argmax(dim=1).item()

# Map label to "YES" or "NO"
label_map = {0: "NO", 1: "YES"}
response = label_map[predicted_label]

print(f"Final Response for the Chunk: {response}")

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load the Longformer tokenizer and model
model_name = "allenai/longformer-base-4096"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Define a function to process a single chunk
def process_chunk(chunk):
    prompt = (
        f"Determine if the following text supports or contradicts the principle of "
        f"'contra proferentem' (a legal doctrine favoring the interpretation against the drafter "
        f"in cases of ambiguous contract terms). Respond with 'YES' if it supports or 'NO' if it contradicts. "
        f"Text:\n\n{chunk}"
    )
    # Tokenize the chunk
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,  # Ensure input fits within the token limit
        padding="max_length",  # Pad to max length for consistency
        max_length=4096
    )

    # Run the model on the tokenized input
    outputs = model(**inputs)

    # Get the logits and predicted label
    logits = outputs.logits
    predicted_label = logits.argmax(dim=1).item()

    # Map label to "YES" or "NO"
    label_map = {0: "NO", 1: "YES"}
    return label_map[predicted_label]

# Process multiple chunks
responses = []
for i, chunk in enumerate(decoded_chunks):  # Assuming `decoded_chunks` contains the text chunks
    print(f"Processing chunk {i+1} of {len(decoded_chunks)}...")
    response = process_chunk(chunk)
    responses.append(response)
    print(f"Chunk {i+1} Response: {response}")

# Combine all responses into a final decision
final_response = "YES" if "YES" in responses else "NO"

print("\nFinal Combined Response for the Document:")
print(final_response)